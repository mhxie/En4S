# En4S

En4S is an enhanced variant of [ReFlex](https://github.com/stanford-mast/reflex) and [ReFlex4ARM](https://github.com/mhxie/reflex4arm.git), building on their legacy to push the boundaries of performance and adaptability, especially for Ephemeral Storage System. ReFlex, as introduced in a [paper](https://web.stanford.edu/group/mast/cgi-bin/drupal/system/files/reflex_asplos17.pdf) published at ASPLOS'17, utilizes Intel's [DPDK](http://dpdk.org) for swift packet processing and [SPDK](http://www.spdk.io) for high-performance NVMe Flash access.

ReFlex is fundamentally a software-based system that enables remote access to Flash storage with performance nearly indistinguishable from local access. It efficiently integrates network and storage processing within a dataplane kernel, achieving both low latency and high throughput with minimal resource consumption. Its innovative I/O scheduler is designed to uphold strict tail latency and throughput service-level objectives (SLOs) for multiple clients sharing a remote Flash device.

Expanding ReFlex's capabilities, ReFlex4ARM adapts the original Intel-based implementation for ARM-based platforms, specifically leveraging the energy-efficient yet powerful Broadcom Stingray Platform. This shift allows for the offloading of tasks to the smart NIC, thus completely freeing the host. ReFlex4ARM's objective is to meld storage, computation, and networking into a cohesive platform that reevaluates the balance between these components. Utilizing an ARM A72 core-based NIC that is both compact and low-power, it proposes a scalable, cost-efficient approach to flash disaggregation in data centers, aiming to reduce both the physical footprint and total cost of ownership (TCO) without significant performance trade-offs.

En4S further evolves to meet the unique demands and opportunities of cloud platforms, notably Amazon EC2. It refines En4S to support the vast number of ephemeral storage flows generated by various cloud clients, including serverless instances. This advancement highlights En4S's commitment to flexibility, efficiency, and scalability, positioning it as a pivotal solution for the dynamic requirements of modern cloud computing.

## Requirements for En4S

En4S requires at least one NVMe Flash device and one Ethernet Card (another for control, if running with remote machine) on the storage target. The client networking interface card should have at least equivalant computing and networking ability.

## AWS EC2 Environments

x86_64 Server:

- Image:
   - Ubuntu Server 20.04 LTS (ami-06e54d05255faf8f6) or
   - Amazon Linux 2 AMI (ami-00f9f4069d04c0c6e)
- Instance: i3-large or i3-xlarge or i3-2xlarge
- Network Interface: 2 \* Elastic Network Adaptor (ENA)
- DPDK/SPDK: v20.07/v20.05

## Setup Instructions

1. Download source code and fetch dependencies:

   ```
   ./get_deps.sh
   ```

2. Install library dependencies:

   ```
   sudo apt-get update && sudo apt-get install libconfig-dev libpciaccess-dev net-tools
   sudo ./spdk/scripts/pkgdep.sh
   ```

   For amazon linux 2 image

   ```
   sudo yum update
   sudo yum install libconfig-devel libpciaccess-devel python3 python3-pip
   sudo yum install kernel-devel-$(uname -r)
   sudo ./spdk/scripts/pkgdep/rhel.sh
   ```

3. Build the dependecies:

   Patch the `spdk/dpdk` with:

   - In `dpdkbuild/Makefile`, append `net/ena` after `DPDK_DRIVERS = bus bus/pci bus/vdev mempool/ring`
   <!-- - In `lib/librte_net/meson.build`, append `rte_ether.h` & `rte_ether.c` -->
   - In `lib/librte_timer/meson.build`, enable `build` as `true`

   Patch the `lwip/src/include/stats.h` with:
   
   - Replace `#include "lwip/memp.h"` with `#include "lwip_if/memp.h"`

   Build SPDK && Drivers

   ```
   cd spdk
   ./configure --with-igb-uio-driver
   sudo make
   ```

4. Build En4S:

   ```
   meson build
   meson compile -C build
   // meson install -C build

   # if you want to clean the build
   rm -r build
   ```

5. Set up the environment:

   ```
   cp ix.conf.sample ix.conf
   # modify at least host_addr, gateway_addr, devices, flow director, and nvme_devices
   # comment out the nvme_devices and modify ns_size at clients

   # sudo modprobe uio
   sudo DRIVER_OVERRIDE=spdk/dpdk/build-tmp/kernel/linux/igb_uio/igb_uio.ko ./spdk/scripts/setup.sh
   grep PCI_SLOT_NAME /sys/class/net/*/device/uevent | awk -F '=' '{print $2}'
   sudo spdk/dpdk/usertools/dpdk-devbind.py --bind=igb_uio 0000:00:04.0 # insert device PCI address here!!!

   sudo sh -c 'for i in /sys/devices/system/node/node*/hugepages/hugepages-2048kB/nr_hugepages; do echo 4096 > $i; done'
   ```

   or simply run on your EC2 instance:

   ```
   ./usertools/conf_setup.sh
   sudo ./usertools/run_reflex_server.sh
   ```

6. Precondition the SSD and derive the request cost model:

   It is necessary to precondition the SSD to acheive steady-state performance and reproducible results across runs. We recommend preconditioning the SSD with the following local Flash tests: write _sequentially_ to the entire address space using 128KB requests, then write _randomly_ to the device with 4KB requests until you see performance reach steady state. The time for the random write test depends on the capacity, use `./perf -h` to check how to configure.

   ```
   cd spdk/examples/nvme/perf
   sudo ./perf -q 1 -s 131072 -w write -t 3600 -c 0x1 -r 'trtype:PCIe traddr:0001:01:00.0'
   ```

   After preconditioning, you should derive the request cost model for your SSD:

   ```
   cp sample.devmodel nvme_devname.devmodel
   # update request costs and token limits to match your SSD performance
   # see instructions in comments of file sample.devmodel
   # in ix.conf file, update nvme_device_model=nvme_devname.devmodel
   ```

   You may use any I/O load generation tool (e.g. [fio](https://github.com/axboe/fio)) for preconditioning and request calibration tests. Note that if you use a Linux-based tool, you will need to reload the nvme kernel module for these tests (remember to unload it before running the ReFlex server).

   For your convenience, we provide an open-loop, local Flash load generator based on the SPDK perf example application [here](https://github.com/anakli/spdk_perf). We modified the SPDK perf example application to report read and write percentile latencies. We also made the load generator open-loop, so you can sweep throughput by specifying a target IOPS instead of queue depth. See setup instructions for ReFlex users in the repository's [README](https://github.com/anakli/spdk_perf/blob/master/README.md).

## Running En4S

### 0. Change some kernel settings (optional):

```
sudo vim /etc/security/limits.conf

*               hard    memlock         unlimited
*               soft    memlock         unlimited
```

### 1. Run the En4S server:

```
sudo ./build/dp
```

ReFlex runs one dataplane thread per CPU core. If you want to run multiple ReFlex threads (to support higher throughput), set the `cpu` list in ix.conf and add `fdir` rules to steer traffic identified by {dest IP, src IP, dest port} to a particular core.

#### Registering service level objectives (SLOs) for ReFlex tenants:

- A _tenant_ is a logical abstraction for accounting for and enforcing SLOs. En4S supports two types of tenants: latency-critical (LC) and best-effort (BE) tenants.
- The current implementation of En4S follows the original design of En4S, which requires tenant SLOs to be specified statically (before running ReFlex) in `pp_accept()` in `dp/core/reflex_server.c`. Each port En4S listens on can be associated with a separate SLO. The tenant should communicate with ReFlex using the destination port that corresponds to the appropriate SLO. This is a temporary implementation until there is proper client API support for a tenant to dynamically register SLOs with ReFlex.

### 2. Run a ReFlex client:

There are several options for clients in the original ReFlex implementations,

#### 2.1 Run an IX-based client that opens TCP connections to ReFlex and sends read/write requests to logical blocks.

Clone En4S source code (userspace branch) on client machine and follow steps 1 to 6 in the setup instructions in userspace branch README. Change the target from `arm64-native-linuxapp-gcc` to `x86_64-native-linuxapp-gcc`. Comment out `nvme_devices` in ix.conf and 
modify `ns_size` at clients.

Single-thread test example:

```
# example: sudo ./build/dp -s 10.10.66.3 -p 1234 -w rand -T 1 -i 100000 -r 100 -S 0 -R 4096 -P 0
```

Multiple-thread test example:

```
# example: sudo ./build/dp -s 10.10.66.3 -p 1234 -w rand -T 4 -i 400000 -r 100 -S 0 -R 4096 -P 0
```

More descriptions for the command line options can be found by running

```
sudo ./build/dp -h

Usage:
sudo ./ix
to run ReFlex server, no parameters required
to run ReFlex client, set the following options
-s  server IP address
-p  server port number
-w  workload type [seq/rand] (default=rand)
-T  number of threads (default=1)
-i  target IOPS for open-loop test (default=50000)
-r  percentage of read requests (default=100)
-S  sweep multiple target IOPS for open-loop test (default=1)
-R  request size in bytes (default=1024)
-P  precondition (default=0)
-d  queue depth for closed-loop test (default=0)
-t  execution time for closed-loop test (default=0)
```

Sample output:

```
RqIOPS:  IOPS:   Avg:    10th:   20th:   30th:   40th:   50th:   60th:   70th:     80th:   90th:   95th:   99th:   max:    missed:
600000   599905  155     106     112     119     126     135     150     167       190     232     274     377     1583    202947
RqIOPS:  IOPS:   Avg:    10th:   20th:   30th:   40th:   50th:   60th:   70th:     80th:   90th:   95th:   99th:   max:    missed:
600000   599913  154     104     111     117     124     133     148     165       189     231     273     378     1578    193293
```

#### 2.2 Run a legacy client application using the ReFlex remote block device driver.

This client option is provided to support legacy applications. ReFlex exposes a standard Linux remote block device interface to the client (which appears to the client as a local block device). The client can mount a filesystem on the block device. With this approach, the client is subject to overheads in the Linux filesystem, block storage layer and network stack.
On the client, change into the reflex_nbd directory and type make. Be sure that the remote ReFlex server is running and that it is ping-able from the client. Load the reflex.ko module, type dmesg to check whether the driver was successfully loaded. To reproduce the fio results from the paper do the following.

_Caveat: This is only for bdev-based tests without file system_

Before begining your tests, change the following parameters at reflex_nbd/reflex_nbd.c:

- `dest_addr` to the destination ip
- `REFLEX_SIZEBYTES` to the `ns_size`
- `dest_cpunr` to the number of threads running at your server
- `submit_queues` to the number of threads running at your client

```
cd reflex_nbd
make

sudo modprobe bnxt_en
 # or try sudo python spdk/dpdk/usertools/dpdk-devbind.py --bind=bnxt_en 01:00.0 && sudo rmmod igb_uio
sudo modprobe nvme
 # make sure you have started reflex_server on the server machine and can ping it

sudo insmod reflex.ko
sudo apt install fio
for i in 1 2 4 8 16 32 ; do BLKSIZE=4k DEPTH=$i fio randread_remote.fio; done
```

## Run En4S in boot time

```
sed -i "s|ubuntu|$USER|g" sample.service
sudo cp sample.service /etc/systemd/system/En4S.service
sudo chmod 644 /etc/systemd/system/En4S.service
sudo systemctl enable En4S
```

## Networking Tuning Guide

Depending on which workload you are using, we suggest the following changes to get the most of the performance out:

### Read-heavy workloads with small request sizes (1k/4k)

1.  For client:

- `inc/lwipopts.h`: TCP_SND_BUF = 4096; TCP_WND = 1 << 16;
- `libix/ixev.h`: IXEV_SEND_DEPTH = 16 \* 2

1.  For server:

- `inc/lwipopts.h`: TCP_SND_BUF = 65536 \* 2; TCP_WND = 1 << 15;
- `libix/ixev.h`: IXEV_SEND_DEPTH = 16 \* 1

### Read-heavy workloads with huge request sizes (64k/128k)

1.  For client:

- `inc/lwipopts.h`: TCP_SND_BUF = 4096; TCP_WND = 1 << 16;
- `libix/ixev.h`: IXEV_SEND_DEPTH = 16 \* 1

1.  For server:

- `inc/lwipopts.h`: TCP_SND_BUF = (65536+24) \* 8; TCP_WND = 1 << 15;
- `libix/ixev.h`: IXEV_SEND_DEPTH = 16 \* 1

Please also check the TCP tuning at [lwIP wiki](https://lwip.fandom.com/wiki/Tuning_TCP).
And change the buffer sizes in the `inc/bufopts.h` according to your application demand.

## Reference

Please refer to our papers [En4S](https://github.com/mhxie/mhxie.github.io/blob/main/assets/paper/En4S.pdf) [1] and [ReFlex4ARM](https://github.com/mhxie/mhxie.github.io/blob/main/assets/paper/ReFlex4ARM.pdf) [2], the original [implementations](https://github.com/stanford-mast/reflex) of ReFlex and its [paper](https://web.stanford.edu/group/mast/cgi-bin/drupal/system/files/reflex_asplos17.pdf) [3]:

[1] Minghao Xie, Chen Qian, Heiner Litz (2024, November). En4S: Enabling SLOs in serverless storage systems. Proceedings of the ACM Symposium on Cloud Computing (SoCC â€™24).

[2] Minghao Xie, Chen Qian, Heiner Litz (2020, January). ReFlex4ARM: Supporting 100GbE Flash Storage Disaggregation on ARM SoC. In OCP Future Technology Symposium.

[3] Ana Klimovic, Heiner Litz, Christos Kozyrakis
ReFlex: Remote Flash == Local Flash
in the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS'22), 2017